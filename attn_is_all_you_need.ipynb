{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Трансформеры: база"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attn.png\" alt=\"Attention is all you need\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План занятий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ЧБДТ: Что Было До Трансформеров\n",
    "   1. При чём тут нейро-лингвистическое программирование?\n",
    "   2. Почему вектора?\n",
    "   3. Счётные методы: CountVectorizer, TF-IDF, добавляем контекст \n",
    "   4. Дистрибутивная семантика: Word2vec, FastText\n",
    "   5. Рекуррентные нейронные сети: LSTM, GRU\n",
    "### 2. Трансформеры: база\n",
    "   1. Мотивация\n",
    "   2. Attention из all you need?: виды attention, интуиция, реализация\n",
    "   3. Архитектура Transformer: эмбеддинги, энкодер, декодер\n",
    "### 3. Трансформеры: на волне хайпа\n",
    "   1. BERT (NLU)\n",
    "   2. GPT (Language Modeling)\n",
    "   3. T5 (Seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прошлые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2v and family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/w2v_properties.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN and family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/rnn_basic.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Out of vocabulary\n",
    "- Слабое понимание контекста (в RNN чуть получше)\n",
    "- Забывание (даже в LSTM)\n",
    "- Нет хороших претрейнутых моделей (как в CV; хотя w2v и ELMO поспорят)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/rnn_vs_transformers.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention (внимание) - на что нам нужно обратить внимание, чтобы понять, что происходит?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/coreference.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аттеншн помогает решить проблему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attn_vis.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Принимает на вход: две последовательности (возможно, одинаковые).\n",
    "- Даёт на выход: вектор \"внимания\", значения которого показывают важность каждого слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention - принимает на вход два одинаковых предложения: делает attention сам на себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим пример из жизни поисковой системы по музыке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_1.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножаем вектор нужного слова на остальные вектора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_2.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Почему это ок? Про косинусное расстояние и иннер продакт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берём софтмакс, чтобы получить веса слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем итоговый вектор слова, взяв взвешенное среднее всех векторов с полученными коэффициентами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_4.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_5.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "navec = Navec.load('navec_hudlit_v1_12B_500K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"включи пожалуйста группу руки вверх\"\n",
    "\n",
    "embeddings = list(map(navec.get, text.split()))\n",
    "embeddings = torch.from_numpy(np.stack(embeddings))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.5143,  9.6992,  2.3823,  0.7420, -1.0193],\n",
       "        [ 9.6992, 36.3740,  2.5938, 11.2488,  6.6931],\n",
       "        [ 2.3823,  2.5938, 35.1974,  3.1319,  4.3334],\n",
       "        [ 0.7420, 11.2488,  3.1319, 32.9065, 13.9943],\n",
       "        [-1.0193,  6.6931,  4.3334, 13.9943, 35.9186]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = embeddings @ embeddings.T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.2346e-10, 8.2006e-14, 1.5902e-14, 2.7324e-15],\n",
       "        [2.6020e-12, 1.0000e+00, 2.1353e-15, 1.2254e-11, 1.2875e-13],\n",
       "        [5.6052e-15, 6.9255e-15, 1.0000e+00, 1.1861e-14, 3.9441e-14],\n",
       "        [1.0743e-14, 3.9280e-10, 1.1723e-13, 1.0000e+00, 6.1167e-09],\n",
       "        [9.0800e-17, 2.0302e-13, 1.9175e-14, 3.0088e-10, 1.0000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_normalized = scores.softmax(dim=1)\n",
    "scores_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00],\n",
       "        [2.6020e-12],\n",
       "        [5.6052e-15],\n",
       "        [1.0743e-14],\n",
       "        [9.0800e-17]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_normalized[:, 0].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.3382e-01, -2.1397e-01,  2.0033e-01, -2.3234e-02, -1.6352e-01,\n",
       "        -3.1473e-01, -2.7189e-01,  1.7513e-01,  4.2780e-01,  3.4569e-01,\n",
       "        -1.8912e-01, -2.5101e-02,  4.4553e-02, -6.3359e-01, -3.4898e-01,\n",
       "         1.6900e-02, -2.2920e-02, -1.6965e-01,  1.5217e-01, -2.2922e-01,\n",
       "        -8.7308e-01,  3.4396e-01, -6.2545e-03,  1.3233e-01, -4.0724e-01,\n",
       "         3.1201e-01,  1.0364e-01, -2.7280e-01, -7.5732e-02,  2.6447e-01,\n",
       "         1.0375e-01, -1.3977e-01, -5.4358e-01, -1.0175e-01,  2.3113e-02,\n",
       "        -1.7922e-01, -1.4222e-01,  2.4688e-01,  7.0777e-01, -1.8071e-01,\n",
       "        -9.9012e-02,  7.4856e-01,  3.4141e-01, -1.3784e-01,  2.0613e-02,\n",
       "         2.7204e-01, -2.9992e-02,  2.3393e-01,  2.8204e-01,  1.0993e-01,\n",
       "        -7.0768e-01, -2.0828e-03,  6.4351e-01, -8.2678e-02, -5.6086e-02,\n",
       "        -4.7781e-03,  7.6802e-03,  2.0830e-01,  2.6347e-01,  4.4920e-02,\n",
       "        -2.3996e-01, -2.0465e-01, -2.7938e-03,  1.9067e-01, -2.5622e-01,\n",
       "        -8.0669e-02, -4.3525e-01, -3.1192e-01, -1.4198e-01,  4.9988e-01,\n",
       "        -4.0323e-01,  4.3141e-01,  7.1234e-02, -1.4738e-01, -3.9386e-01,\n",
       "         7.4503e-01,  9.1986e-02, -3.5834e-01,  8.2952e-03, -1.2249e-01,\n",
       "         3.9782e-01,  2.3753e-01,  4.8266e-02,  3.8048e-02, -2.5921e-01,\n",
       "        -3.4761e-01,  3.5390e-01,  2.1228e-01,  6.7090e-01, -1.0233e-01,\n",
       "        -1.7145e-01,  5.7884e-04,  9.3279e-02, -3.1045e-01,  1.3382e-01,\n",
       "        -6.9179e-01, -5.0621e-01,  6.4212e-02,  1.5181e-01,  2.7688e-01,\n",
       "        -8.0630e-02,  5.2215e-01, -3.0824e-01,  7.7535e-03, -9.2800e-02,\n",
       "        -6.6128e-01, -7.2212e-02,  1.1827e-01,  3.0103e-01,  1.1575e-01,\n",
       "        -4.4328e-01, -1.4468e-02,  2.2116e-01, -2.3716e-01,  4.0888e-01,\n",
       "         1.4221e-01,  1.5173e-01, -5.0776e-01,  3.2441e-02, -2.2843e-01,\n",
       "         7.5561e-01,  3.6742e-01, -4.1517e-01, -9.4456e-02, -3.7519e-01,\n",
       "        -2.1261e-01, -1.4089e-01, -4.2966e-01,  2.5179e-01,  1.7456e-01,\n",
       "        -2.1756e-01,  1.2615e-01, -5.1072e-01, -2.1120e-01,  4.8209e-01,\n",
       "        -4.6775e-01, -1.2149e-01, -1.6632e-01, -3.4794e-01,  3.0213e-01,\n",
       "         4.8748e-01,  9.7564e-02,  3.9462e-02,  5.8507e-02,  2.9396e-01,\n",
       "         7.1365e-02,  9.4007e-02, -4.1705e-01,  1.7641e-01,  1.3802e-01,\n",
       "        -1.9662e-01,  2.7873e-01,  4.5834e-01, -7.6910e-01,  1.1923e-03,\n",
       "         5.1584e-02, -8.8472e-02, -1.5118e-01,  3.9486e-01, -1.5707e-02,\n",
       "        -1.5658e-01, -8.7794e-01, -1.7304e-01, -3.7081e-01, -2.4187e-01,\n",
       "        -8.9770e-02,  8.6426e-02, -1.0822e-01,  4.4875e-01,  4.0269e-01,\n",
       "        -1.0891e-01,  4.8981e-01,  2.3838e-03, -7.8314e-01,  1.8306e-01,\n",
       "        -3.2497e-01, -3.6040e-01, -4.2189e-02,  3.7156e-01,  2.3572e-01,\n",
       "        -3.0925e-01, -3.3337e-01,  4.5020e-01,  1.4312e-01,  2.9623e-01,\n",
       "        -2.3606e-01,  2.7424e-01,  2.0113e-01,  7.6490e-02,  3.8162e-01,\n",
       "        -3.6624e-01, -7.5843e-01,  4.0129e-01,  5.0226e-01, -1.4533e-02,\n",
       "        -6.6501e-02, -6.5980e-01, -1.4951e-01,  1.0722e-01,  2.2604e-01,\n",
       "        -1.0189e-01,  5.5809e-01, -1.4230e-01,  1.9141e-01, -2.5951e-01,\n",
       "        -4.9767e-01,  6.0636e-01,  3.7318e-01, -9.8247e-02, -2.3749e-01,\n",
       "         2.9003e-01,  9.3371e-02, -1.1998e-01,  4.4002e-01, -1.4999e-01,\n",
       "         5.5744e-01, -4.2791e-01, -5.1760e-01, -2.8691e-01, -2.2956e-01,\n",
       "        -3.7416e-01, -2.4501e-01,  3.4398e-02,  3.3477e-01,  7.2749e-01,\n",
       "         4.4074e-01,  1.5081e-02, -4.8844e-02, -2.5791e-01, -3.3864e-01,\n",
       "         5.3844e-02, -5.6416e-01,  1.7874e-01,  3.1698e-02, -2.8905e-01,\n",
       "         3.6333e-01,  4.2131e-01, -1.4317e-01,  3.7802e-01,  3.6880e-01,\n",
       "         2.5618e-01, -1.2323e-01, -2.8012e-01, -1.6226e-01, -2.0323e-01,\n",
       "        -3.4675e-01, -2.7426e-01,  6.4693e-01,  7.9388e-02, -8.8843e-02,\n",
       "        -3.2431e-01, -1.2434e-01, -2.0448e-01,  6.6222e-02, -3.7475e-01,\n",
       "         2.4307e-01,  2.9855e-01, -5.6999e-01, -2.5472e-02,  1.9701e-01,\n",
       "        -1.5409e-01, -4.5870e-01,  7.6439e-02,  2.9502e-02, -8.1431e-02,\n",
       "         1.0100e-01,  3.3618e-01,  2.5099e-03,  1.1074e-01,  3.1263e-01,\n",
       "         2.4695e-02, -7.4106e-01,  3.9362e-01, -3.5748e-01, -7.9760e-02,\n",
       "         5.1753e-01, -6.3042e-01,  3.9915e-01,  9.5095e-02, -5.1779e-01,\n",
       "        -4.2582e-01, -4.4741e-02,  8.4866e-02,  3.6786e-02,  4.4457e-01,\n",
       "        -4.4166e-02, -5.9811e-02,  1.9196e-01,  8.9529e-02,  2.5300e-01,\n",
       "         1.3775e-01,  8.4396e-01,  3.6433e-01,  3.4756e-01, -4.5217e-02,\n",
       "        -4.3838e-01, -2.4100e-01,  5.8756e-01, -3.8662e-01, -1.2876e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_emb = (embeddings * scores_normalized[:, 0].view(-1, 1)).sum(dim=0)\n",
    "first_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_emb == embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_embeddings = embeddings.repeat(embeddings.shape[0], 1, 1)\n",
    "repeated_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8338, -0.2140,  0.2003,  ...,  0.5876, -0.3866, -0.1288],\n",
       "        [ 0.8338, -0.2140,  0.2003,  ...,  0.5876, -0.3866, -0.1288],\n",
       "        [ 0.8338, -0.2140,  0.2003,  ...,  0.5876, -0.3866, -0.1288],\n",
       "        [ 0.8338, -0.2140,  0.2003,  ...,  0.5876, -0.3866, -0.1288],\n",
       "        [ 0.8338, -0.2140,  0.2003,  ...,  0.5876, -0.3866, -0.1288]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_embeddings[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 5, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuted_embeddings = repeated_embeddings.permute(2, 0, 1)\n",
    "permuted_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_attented = (permuted_embeddings * scores_normalized).sum(dim=2).permute(1, 0)\n",
    "embeddings_attented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_attented == embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тут пришло осознание..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scores_normalized @ embeddings) == embeddings_attented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_self_attn(embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"embeddings (seq_len, emb_dim)\"\"\"\n",
    "    scores = embeddings @ embeddings.T  # (seq_len, seq_len)\n",
    "    scores_normalized = scores.softmax(dim=1)  # (seq_len, seq_len)\n",
    "    \n",
    "    # repeated_embeddings = embeddings.repeat(embeddings.shape[0], 1, 1)  # (seq_len, seq_len, emb_dim)\n",
    "    # permuted_embeddings = repeated_embeddings.permute(2, 0, 1)  # (emb_dim, seq_len, seq_len)\n",
    "\n",
    "    # embeddings_attented = (permuted_embeddings * scores_normalized).sum(dim=2).permute(1, 0)  # (seq_len, emb_dim)\n",
    "\n",
    "    embeddings_attented = scores_normalized @ embeddings\n",
    "\n",
    "    return embeddings_attented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_self_attn(embeddings).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_full.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Весёлое упражнение:** батчизовать! Принимать на вход и возвращать `(B, seq_len, emb_dim)`. Без циклов, конечно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема: сильно слабый, не годится для хорошего понимания контекста.\n",
    "\n",
    "Для чего он может быть хорош?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot-product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query - что хотим найти? (токен, для которого хотим посчитать аттеншн)\n",
    "- Key - что во мне содержится? (токены, на которые будем смотреть)\n",
    "- Value - какая информация тебе нужна? (значения, которые будем получать)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attn_intuition.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query, Key, Value - обучаемые матрицы (по сути, обычные линейные слои)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attn_matrix_base.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая формула:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attention_matrices.webp\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attn_formula.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain scaling!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.7311])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.,2.]).softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"руки вверх восемнадцать мне уже\"\n",
    "\n",
    "embeddings2 = list(map(navec.get, text2.split()))\n",
    "embeddings2 = torch.from_numpy(np.stack(embeddings2))\n",
    "embeddings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7420, -1.0193,  0.5002,  1.5011, -0.6839],\n",
       "        [11.2488,  6.6931,  5.1217, 15.4657,  6.2644],\n",
       "        [ 3.1319,  4.3334,  4.2619,  3.9792,  4.6229],\n",
       "        [32.9065, 13.9943,  1.9484, 10.9348,  8.9146],\n",
       "        [13.9943, 35.9186,  3.1689,  4.2400,  5.2509]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings @ embeddings2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4641)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.tensor(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.scale = torch.sqrt(torch.tensor(emb_size))\n",
    "\n",
    "        self.q_w = nn.Linear(emb_size, emb_size)\n",
    "        self.k_w = nn.Linear(emb_size, emb_size)\n",
    "        self.v_w = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        xq = self.q_w(x)\n",
    "        yk = self.k_w(y)\n",
    "        yv = self.v_w(y)\n",
    "\n",
    "        attn = torch.softmax((xq @ yk.T) / self.scale, dim=-1) @ yv\n",
    "\n",
    "        return attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(embeddings, embeddings2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot-product Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/self_attn_mask.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/multihead_attn.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/transformer_architecture.webp\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/attn_order.avif\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/second/thanks_attention.png\" width=\"1200\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
