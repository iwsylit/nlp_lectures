{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ЧБДТ: Что Было До Трансформеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/nlp_before.png\" alt=\"NLP before meme\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План занятий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ЧБДТ: Что Было До Трансформеров\n",
    "   1. При чём тут нейро-лингвистическое программирование?\n",
    "   2. Почему вектора?\n",
    "   3. Счётные методы: CountVectorizer, TF-IDF, добавляем контекст \n",
    "   4. Дистрибутивная семантика: Word2vec, FastText\n",
    "   5. Рекуррентные нейронные сети: LSTM, GRU\n",
    "### 2. Трансформеры: база\n",
    "   1. Мотивация\n",
    "   2. Attention из all you need?: виды attention, интуиция, реализация\n",
    "   3. Архитектура Transformer: эмбеддинги, энкодер, декодер\n",
    "### 3. Трансформеры: на волне хайпа\n",
    "   1. BERT\n",
    "   2. GPT\n",
    "   3. T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткое введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Где используется NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поиск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/nlp_examples.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чат-боты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/ChatGPT.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переводчики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/translator.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приложения для изучения языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/duolingo.webp\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/reverso.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Суммаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/Summarizer.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Есть проблемы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная проблема всея ML - классификация. Это верно и для NLP.\n",
    "\n",
    "Например:\n",
    "1. Спам или что-то осмысленное?\n",
    "2. Позитивное или негативное?\n",
    "3. К какой теме относится?\n",
    "4. Что от нас хочет пользователь?\n",
    "5. Токсичность, оскорбления или ок?\n",
    "\n",
    "Нужно как-то заставить компьютер понимать язык."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему вектора?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/vectors.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Плюсы\n",
    "1. Понятно для компьютера\n",
    "2. Хорошо умеем с ними работать (математика + машинное обучение)\n",
    "3. Можем заложить знания о языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Минусы\n",
    "1. Большое упрощение\n",
    "2. Неинтерпретируемо (иногда)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поехали..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install openpyxl pandas torch transformers pymorphy3 pymorphy3-dicts-ru navec\n",
    "%pip install scikit-learn nltk matplotlib seaborn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget -nc https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import nltk; nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Счётные методы + предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основная идея\n",
    "Считаем все слова в тексте; представляем текст как вектор (1 х V), где V - размер словаря (количество уникальных слов во всех текстах выборки). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например: Моя мама мыла раму. Раму мыла мама, мама.\n",
    "\n",
    "||моя|мама|мыла|раму|\n",
    "|-|-|-|-|-|\n",
    "|1.|1|1|1|1|\n",
    "|2.|0|2|1|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В scikit-learn для этого существует `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_dataset = [\"моя мама мыла раму\", \"раму мыла мама, мама\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Токенизатор** - это программа или алгоритм, который разбивает текст на отдельные единицы, называемые токенами. \\\n",
    "Токены могут быть словами, символами, знаками препинания или другими элементами, которые важны для нашей задачи.\n",
    "\n",
    "Напишем свой собственный токенизатор!\n",
    "\n",
    "*(а можно было взять уже готовый из [razdel](https://github.com/natasha/razdel), spacy, nltk, ...)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/tokenizers.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_regex = re.compile(r\"[\\w]+\")\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return re.findall(tokenization_regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['моя', 'мама', 'мыла', 'раму'], ['раму', 'мыла', 'мама', 'мама']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_micro_dataset = list(map(tokenize, micro_dataset))\n",
    "tokenized_micro_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.int64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Convert a collection of text documents to a matrix of token counts.\n",
      "\n",
      "This implementation produces a sparse representation of the counts using\n",
      "scipy.sparse.csr_matrix.\n",
      "\n",
      "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      "that does some kind of feature selection then the number of features will\n",
      "be equal to the vocabulary size found by analyzing the data.\n",
      "\n",
      "For an efficiency comparision of the different feature extractors, see\n",
      ":ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
      "\n",
      "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input : {'filename', 'file', 'content'}, default='content'\n",
      "    - If `'filename'`, the sequence passed as an argument to fit is\n",
      "      expected to be a list of filenames that need reading to fetch\n",
      "      the raw content to analyze.\n",
      "\n",
      "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "      object) that is called to fetch the bytes in memory.\n",
      "\n",
      "    - If `'content'`, the input is expected to be a sequence of items that\n",
      "      can be of type string or byte.\n",
      "\n",
      "encoding : str, default='utf-8'\n",
      "    If bytes or files are given to analyze, this encoding is used to\n",
      "    decode.\n",
      "\n",
      "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "    Instruction on what to do if a byte sequence is given to analyze that\n",
      "    contains characters not of the given `encoding`. By default, it is\n",
      "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "    values are 'ignore' and 'replace'.\n",
      "\n",
      "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      "    Remove accents and perform other character normalization\n",
      "    during the preprocessing step.\n",
      "    'ascii' is a fast method that only works on characters that have\n",
      "    a direct ASCII mapping.\n",
      "    'unicode' is a slightly slower method that works on any characters.\n",
      "    None (default) means no character normalization is performed.\n",
      "\n",
      "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "    :func:`unicodedata.normalize`.\n",
      "\n",
      "lowercase : bool, default=True\n",
      "    Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "preprocessor : callable, default=None\n",
      "    Override the preprocessing (strip_accents and lowercase) stage while\n",
      "    preserving the tokenizing and n-grams generation steps.\n",
      "    Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "tokenizer : callable, default=None\n",
      "    Override the string tokenization step while preserving the\n",
      "    preprocessing and n-grams generation steps.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "stop_words : {'english'}, list, default=None\n",
      "    If 'english', a built-in stop word list for English is used.\n",
      "    There are several known issues with 'english' and you should\n",
      "    consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "    If a list, that list is assumed to contain stop words, all of which\n",
      "    will be removed from the resulting tokens.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    If None, no stop words will be used. In this case, setting `max_df`\n",
      "    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      "    and filter stop words based on intra corpus document frequency of terms.\n",
      "\n",
      "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "    Regular expression denoting what constitutes a \"token\", only used\n",
      "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      "    or more alphanumeric characters (punctuation is completely ignored\n",
      "    and always treated as a token separator).\n",
      "\n",
      "    If there is a capturing group in token_pattern then the\n",
      "    captured group content, not the entire match, becomes the token.\n",
      "    At most one capturing group is permitted.\n",
      "\n",
      "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "    The lower and upper boundary of the range of n-values for different\n",
      "    word n-grams or char n-grams to be extracted. All values of n such\n",
      "    such that min_n <= n <= max_n will be used. For example an\n",
      "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      "    Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "    Whether the feature should be made of word n-gram or character\n",
      "    n-grams.\n",
      "    Option 'char_wb' creates character n-grams only from text inside\n",
      "    word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "    If a callable is passed it is used to extract the sequence of features\n",
      "    out of the raw, unprocessed input.\n",
      "\n",
      "    .. versionchanged:: 0.21\n",
      "\n",
      "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "    first read from the file and then passed to the given callable\n",
      "    analyzer.\n",
      "\n",
      "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly higher than the given threshold (corpus-specific\n",
      "    stop words).\n",
      "    If float, the parameter represents a proportion of documents, integer\n",
      "    absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "min_df : float in range [0.0, 1.0] or int, default=1\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly lower than the given threshold. This value is also\n",
      "    called cut-off in the literature.\n",
      "    If float, the parameter represents a proportion of documents, integer\n",
      "    absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "max_features : int, default=None\n",
      "    If not None, build a vocabulary that only consider the top\n",
      "    `max_features` ordered by term frequency across the corpus.\n",
      "    Otherwise, all features are used.\n",
      "\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "vocabulary : Mapping or iterable, default=None\n",
      "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "    indices in the feature matrix, or an iterable over terms. If not\n",
      "    given, a vocabulary is determined from the input documents. Indices\n",
      "    in the mapping should not be repeated and should not have any gap\n",
      "    between 0 and the largest index.\n",
      "\n",
      "binary : bool, default=False\n",
      "    If True, all non zero counts are set to 1. This is useful for discrete\n",
      "    probabilistic models that model binary events rather than integer\n",
      "    counts.\n",
      "\n",
      "dtype : dtype, default=np.int64\n",
      "    Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "vocabulary_ : dict\n",
      "    A mapping of terms to feature indices.\n",
      "\n",
      "fixed_vocabulary_ : bool\n",
      "    True if a fixed vocabulary of term to indices mapping\n",
      "    is provided by the user.\n",
      "\n",
      "stop_words_ : set\n",
      "    Terms that were ignored because they either:\n",
      "\n",
      "      - occurred in too many documents (`max_df`)\n",
      "      - occurred in too few documents (`min_df`)\n",
      "      - were cut off by feature selection (`max_features`).\n",
      "\n",
      "    This is only available if no vocabulary was given.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "HashingVectorizer : Convert a collection of text documents to a\n",
      "    matrix of token counts.\n",
      "\n",
      "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
      "    of TF-IDF features.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The ``stop_words_`` attribute can get large and increase the model size\n",
      "when pickling. This attribute is provided only for introspection and can\n",
      "be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
      ">>> corpus = [\n",
      "...     'This is the first document.',\n",
      "...     'This document is the second document.',\n",
      "...     'And this is the third one.',\n",
      "...     'Is this the first document?',\n",
      "... ]\n",
      ">>> vectorizer = CountVectorizer()\n",
      ">>> X = vectorizer.fit_transform(corpus)\n",
      ">>> vectorizer.get_feature_names_out()\n",
      "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "       'this'], ...)\n",
      ">>> print(X.toarray())\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      ">>> X2 = vectorizer2.fit_transform(corpus)\n",
      ">>> vectorizer2.get_feature_names_out()\n",
      "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
      "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
      "       'this document', 'this is', 'this the'], ...)\n",
      " >>> print(X2.toarray())\n",
      " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/ml/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     TfidfVectorizer"
     ]
    }
   ],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>мама</th>\n",
       "      <th>моя</th>\n",
       "      <th>мыла</th>\n",
       "      <th>раму</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   мама  моя  мыла  раму\n",
       "0     1    1     1     1\n",
       "1     2    0     1     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "vectors = vectorizer.fit_transform(tokenized_micro_dataset)\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какая проблема будет для предложения \"Мы увидели зеленый стол, подошли к зеленому столу, пододвинули другие зелёные столы.\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация\n",
    "**Лемматизация** - приведение слов в нормальную форму. \\\n",
    "Нормальная форма обычно является именительным падежом единственного числа для существительных, инфинитивом для глаголов, мужским родом для прилагательных и т.д. Лемматизация учитывает морфологические и синтаксические особенности языка и пытается сохранить смысл слова. \n",
    "\n",
    "Есть ещё **стемминг** - удаление суффиксов/окончаний, чтобы получить его основу или корень. \\\n",
    "Основа или корень не обязательно является нормальной формой или даже существующим словом, а просто общим элементом для группы слов.\n",
    "\n",
    "Лемматизированный текст: мы увидеть зеленый стол, подойти к зеленый стол, пододвинуть другой зеленый стол. \\\n",
    "Стеммированный текст: мы увид зелен стол, подошл к зелен стол, пододвин друг зелен стол."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского обычно используют лемматизацию. В питоне с помощью библиотек: pymorphy2/3, natasha, rmmnorph, spacy, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(word: str) -> str:\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['мой', 'мама', 'мыло', 'рама'], ['рама', 'мыло', 'мама', 'мама']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: list(map(lemmatize, x)), tokenized_micro_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какая будет проблема, если тексты будут очень разной длины?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема - частоты слов будут несопоставимы. \\\n",
    "Решение - используем бинарный векторайзер: ставим 1, если слово встретилось хотя бы один раз; ставим 0, если слова не было в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>мама</th>\n",
       "      <th>моя</th>\n",
       "      <th>мыла</th>\n",
       "      <th>раму</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   мама  моя  мыла  раму\n",
       "0     1    1     1     1\n",
       "1     1    0     1     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False, binary=True)\n",
    "vectors = vectorizer.fit_transform(tokenized_micro_dataset)\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие ещё проблемы будут?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсказка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Закон Ципфа\n",
    "\n",
    "Частота слова в языке обратно пропорциональна порядковому номеру этого слова.\n",
    "\n",
    "<img src=\"attachements/first/zipf.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема - самые частотные слова будут встречаться чаще всего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cтоп-слова - слова, которые бесполезны для решения задачи, мусорные слова. Часто - достаточно частотные в языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'что', 'он', 'на', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "\n",
    "stopwords = stopwords_nltk.words('russian')\n",
    "stopwords = [w for w in stopwords if w not in ['не', 'нет', 'я']]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_meaningful(word: str) -> bool:\n",
    "    return word not in stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Финальный пайплайн предобработки текста:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Привести к нижнему регистру\n",
    "2. Токенизировать\n",
    "3. Лемматизировать\n",
    "4. Убрать стоп-слова\n",
    "\n",
    "В зависимости от вашей задачи и используемых методов - пайплайн может меняться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    tokens = tokenize(text)\n",
    "    lemmas = map(lemmatize, tokens)\n",
    "    lemmas_meaningful = filter(is_meaningful, lemmas)\n",
    "\n",
    "    return list(lemmas_meaningful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['зелёный', 'трава', 'зеленить', 'зелёный', 'трава']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Зелёная трава зеленила зелёную траву!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея: перевзвесить слова таким образом, чтобы частотным словам (которые встречаются в большом количестве документов)\n",
    "придать вес поменьше, а редким (специфическим для определенных текстов) словам - побольше.\n",
    "\n",
    "<img src=\"attachements/first/tfidf.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему в формуле логарифм?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>мама</th>\n",
       "      <th>моя</th>\n",
       "      <th>мыла</th>\n",
       "      <th>раму</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.448321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       мама       моя      мыла      раму\n",
       "0  0.448321  0.630099  0.448321  0.448321\n",
       "1  0.816497  0.000000  0.408248  0.408248"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "vectors = vectorizer.fit_transform(tokenized_micro_dataset)\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['моя мама мыла раму', 'раму мыла мама, мама']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно TF-IDF оказывается лучше других счётных методов, но иногда и другие стреляют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли ещё проблемы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсказка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как будут представлены предложения \"я тебя не люблю\" и \"я люблю не тебя\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем контекстуальность через би-, три- и тд-граммы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/ngrams.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>мама</th>\n",
       "      <th>мама мама</th>\n",
       "      <th>мама мыла</th>\n",
       "      <th>мама мыла раму</th>\n",
       "      <th>моя</th>\n",
       "      <th>моя мама</th>\n",
       "      <th>моя мама мыла</th>\n",
       "      <th>мыла</th>\n",
       "      <th>мыла мама</th>\n",
       "      <th>мыла мама мама</th>\n",
       "      <th>мыла раму</th>\n",
       "      <th>раму</th>\n",
       "      <th>раму мыла</th>\n",
       "      <th>раму мыла мама</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   мама  мама мама  мама мыла  мама мыла раму  моя  моя мама  моя мама мыла  \\\n",
       "0     1          0          1               1    1         1              1   \n",
       "1     2          1          0               0    0         0              0   \n",
       "\n",
       "   мыла  мыла мама  мыла мама мама  мыла раму  раму  раму мыла  раму мыла мама  \n",
       "0     1          0               0          1     1          0               0  \n",
       "1     1          1               1          0     1          1               1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False, ngram_range=(1, 3))\n",
    "vectors = vectorizer.fit_transform(tokenized_micro_dataset)\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как будем решать проблему с огромной размерностью получившейся матрицы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обрезать совсем редкие н-граммы или токены\n",
    "2. Снижать размерность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.txt\")as file:\n",
    "    data = file.read().split(\"\\n\")[:-1]\n",
    "    data = list(map(lambda x: x.split(\" \", maxsplit=1), data))\n",
    "    data = pd.DataFrame(data, columns=[\"target\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__INSULT</td>\n",
       "      <td>скотина! что сказать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__NORMAL</td>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__NORMAL</td>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__NORMAL</td>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__NORMAL</td>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target                                               text\n",
       "0  __label__INSULT                               скотина! что сказать\n",
       "1  __label__NORMAL  я сегодня проезжала по рабочей и между домами ...\n",
       "2  __label__NORMAL  очередной лохотрон. зачем придумывать очередно...\n",
       "3  __label__NORMAL  ретро дежавю ... сложно понять чужое сердце , ...\n",
       "4  __label__NORMAL            а когда мы статус агрогородка получили?"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "__label__NORMAL                                       203685\n",
       "__label__INSULT                                        28567\n",
       "__label__INSULT,__label__THREAT                         6317\n",
       "__label__THREAT                                         5460\n",
       "__label__OBSCENITY                                      2245\n",
       "__label__INSULT,__label__OBSCENITY                      1766\n",
       "__label__INSULT,__label__OBSCENITY,__label__THREAT       176\n",
       "__label__OBSCENITY,__label__THREAT                        74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target\"] = data[\"target\"] \\\n",
    "    .apply(lambda x: \"__label__OBSCENITY\" if \"OBSCENITY\" in x else x) \\\n",
    "    .apply(lambda x: \"__label__THREAT\" if \"THREAT\" in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "__label__NORMAL       203685\n",
       "__label__INSULT        28567\n",
       "__label__THREAT        11777\n",
       "__label__OBSCENITY      4261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).groupby(\"target\").head(6666).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "__label__INSULT       6666\n",
       "__label__NORMAL       6666\n",
       "__label__THREAT       6666\n",
       "__label__OBSCENITY    4261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target\"] = data[\"target\"].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>это твоя мама толстая все сжирает,а у путьки н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>правда на 100% . прежде чем что то требовать н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>все правильно не уступай трамваю, и поездам то...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>таких людей убивать нужно, бедный ребёнок, сер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>опять театр опять развлекуха!! опять клоуны, а...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  это твоя мама толстая все сжирает,а у путьки н...\n",
       "1       1  правда на 100% . прежде чем что то требовать н...\n",
       "2       0  все правильно не уступай трамваю, и поездам то...\n",
       "3       2  таких людей убивать нужно, бедный ребёнок, сер...\n",
       "4       1  опять театр опять развлекуха!! опять клоуны, а..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24259 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24259/24259 [00:19<00:00, 1246.75it/s]\n"
     ]
    }
   ],
   "source": [
    "data[\"processed_text\"] = data[\"text\"].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>это твоя мама толстая все сжирает,а у путьки н...</td>\n",
       "      <td>[это, твой, мама, толстой, всё, сжирать, путьк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>правда на 100% . прежде чем что то требовать н...</td>\n",
       "      <td>[правда, 100, прежде, требовать, дать, вложиться]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>все правильно не уступай трамваю, и поездам то...</td>\n",
       "      <td>[всё, правильно, не, уступать, трамвай, поезд,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>таких людей убивать нужно, бедный ребёнок, сер...</td>\n",
       "      <td>[человек, убивать, нужно, бедный, ребёнок, сер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>опять театр опять развлекуха!! опять клоуны, а...</td>\n",
       "      <td>[театр, развлекуха, клоун, рабочий, класс, сел...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  это твоя мама толстая все сжирает,а у путьки н...   \n",
       "1       1  правда на 100% . прежде чем что то требовать н...   \n",
       "2       0  все правильно не уступай трамваю, и поездам то...   \n",
       "3       2  таких людей убивать нужно, бедный ребёнок, сер...   \n",
       "4       1  опять театр опять развлекуха!! опять клоуны, а...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [это, твой, мама, толстой, всё, сжирать, путьк...  \n",
       "1  [правда, 100, прежде, требовать, дать, вложиться]  \n",
       "2  [всё, правильно, не, уступать, трамвай, поезд,...  \n",
       "3  [человек, убивать, нужно, бедный, ребёнок, сер...  \n",
       "4  [театр, развлекуха, клоун, рабочий, класс, сел...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24259.000000\n",
       "mean         8.337483\n",
       "std          9.629260\n",
       "min          0.000000\n",
       "50%          5.000000\n",
       "99%         51.000000\n",
       "max        155.000000\n",
       "Name: processed_text, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"processed_text\"].str.len().describe(percentiles=[0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"processed_text\"].apply(len) >= 3].reset_index(drop=True)\n",
    "data[\"processed_text\"] = data[\"processed_text\"].apply(lambda x: x[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    5714\n",
       "2    5502\n",
       "1    5385\n",
       "3    3580\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.15, stratify=data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_data[\"processed_text\"].values, test_data[\"processed_text\"] \n",
    "y_train, y_test = train_data[\"target\"].values, test_data[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=(1, 1), \n",
    "        max_features=10000,  # Взяли с потолка!\n",
    "        tokenizer=lambda x: x, \n",
    "        lowercase=False\n",
    "    )),\n",
    "    ('svm',  LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(lowercase=False, max_features=10000,\n",
       "                                 tokenizer=&lt;function &lt;lambda&gt; at 0x296c38a40&gt;)),\n",
       "                (&#x27;svm&#x27;, LinearSVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(lowercase=False, max_features=10000,\n",
       "                                 tokenizer=&lt;function &lt;lambda&gt; at 0x296c38a40&gt;)),\n",
       "                (&#x27;svm&#x27;, LinearSVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False, max_features=10000,\n",
       "                tokenizer=&lt;function &lt;lambda&gt; at 0x296c38a40&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(lowercase=False, max_features=10000,\n",
       "                                 tokenizer=<function <lambda> at 0x296c38a40>)),\n",
       "                ('svm', LinearSVC())])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred, average='macro'):\n",
    "    precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "\n",
    "    print(f'{average.capitalize()} Precision = {precision:.4f}, Recall = {recall:.4f}, F1 = {f1:.4f}')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.9706, Recall = 0.9702, F1 = 0.9704\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(y_train, pred_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.8261, Recall = 0.8224, F1 = 0.8238\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(y_test, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дистрибутивная семантика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача\n",
    "Получить \"экспрессивные\" вектора для слов.\n",
    "### Идея\n",
    "Значение слова определяется его контекстом. Значит, нужно заложить информацию о возможных контекстах в вектор слова.\n",
    "\n",
    "<img src=\"attachements/first/distributional_semantics.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронная сеть, параметры которой - вектора слов. Вектора обновляются итеративно, как при обучении любой нейронки.\n",
    "\n",
    " - Учим модель предсказывать слова контекста по центральному слову (было слово в контексте или нет) - **Skip-gram**\n",
    " - Либо наоборот! Предсказывать слово по контексту (было слово в центре или нет) - **CBOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пайплайн обучения:\n",
    "* Берём огромный корпус текстов (без разметки - self-supervised learning!)\n",
    "* Шагаем по текстам, слово за словом. На каждом шаге смотрим на центральное слово и контекст (заданного размера)\n",
    "* Смотря на центральное слово, считаем вероятности того, что слово из контекста находится радом с центральным словом - **Skip-gram**\n",
    "  * Либо наоборот! Смотрим на слова контекста, считаем вероятности того, что центральное слово находится рядом с контекстами - **CBOW**\n",
    "* Изменяем вектора, чтобы увеличить эту вероятность (бэк-проп, градиентный спуск, все дела)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/w2v_training.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лосс\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW vs Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/cbow_skipgram.webp\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос\n",
    "Какие преимущества у каждого из методов в каких ситуациях?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Плюсы CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Лучше для частотных слов (почему?)\n",
    "- Лучше учит синтаксические связи (кот и коты ближе друг к другу)\n",
    "- Учится быстрее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Плюсы Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Меньше переобучается на частотные слова\n",
    "- Лучше выражает редкие слова\n",
    "- Требует меньше данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интуиция и прикольные свойства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/w2v_properties.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "\n",
    "navec = Navec.load('navec_hudlit_v1_12B_500K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "navec_embeddings = torch.from_numpy(np.stack(list(map(navec.get, navec.vocab.words[:-2]))))\n",
    "navec_embeddings = torch.nn.functional.normalize(navec_embeddings, dim=-1)\n",
    "\n",
    "idx_to_word = dict(enumerate(navec.vocab.words[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, top_k=5):\n",
    "    embedding = torch.nn.functional.normalize(torch.from_numpy(navec[word]), dim=0)\n",
    "    \n",
    "    sim = embedding @ navec_embeddings.T\n",
    "    idx = sim.topk(top_k + 1).indices\n",
    "\n",
    "    top_sim = sim[idx][1:].tolist()\n",
    "    top_words = [idx_to_word[i] for i in idx.tolist()[1:]]\n",
    "\n",
    "    return pd.DataFrame(zip(top_words, top_sim), columns=[\"word\", \"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>кошка</td>\n",
       "      <td>0.671421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>котенок</td>\n",
       "      <td>0.656233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>пес</td>\n",
       "      <td>0.650454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>кота</td>\n",
       "      <td>0.646174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>рыжий</td>\n",
       "      <td>0.611956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  similarity\n",
       "0    кошка    0.671421\n",
       "1  котенок    0.656233\n",
       "2      пес    0.650454\n",
       "3     кота    0.646174\n",
       "4    рыжий    0.611956"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest(\"кот\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>дождик</td>\n",
       "      <td>0.670390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>моросящий</td>\n",
       "      <td>0.615993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>дождичек</td>\n",
       "      <td>0.612513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>моросил</td>\n",
       "      <td>0.574571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дождь</td>\n",
       "      <td>0.557204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  similarity\n",
       "0     дождик    0.670390\n",
       "1  моросящий    0.615993\n",
       "2   дождичек    0.612513\n",
       "3    моросил    0.574571\n",
       "4      дождь    0.557204"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest(\"мелкий\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_three():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7520469"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navec.sim(\"постель\", \"кровать\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### За и против"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Плюсы\n",
    "- Репрезентации слов, понимающие отношения и характеристики реального мира\n",
    "- Можно брать и использовать!\n",
    "\n",
    "#### Минусы\n",
    "- Неинтерпретируемы\n",
    "- Есть проблемы с разными значениями слов\n",
    "- Есть out-of-vocabulary слова: новые слова, редкие слова, опечатки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Козыри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что если брать эмбеддинги последовательностей букв?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachements/first/fasttext.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема с out-of-vocabulary частично решается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text: list[str], mean=True) -> np.ndarray:\n",
    "    embeddings = [navec.get(x, navec['<unk>']) for x in text]\n",
    "    \n",
    "    if mean:\n",
    "        return np.stack(embeddings).mean(axis=0)\n",
    "    else:\n",
    "        return np.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(train_data[\"processed_text\"].apply(vectorize))\n",
    "X_test = np.stack(test_data[\"processed_text\"].apply(vectorize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17153, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=300)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = logreg.predict(X_train)\n",
    "pred_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.7809, Recall = 0.7689, F1 = 0.7734\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(y_train, pred_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.7412, Recall = 0.7325, F1 = 0.7360\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(y_test, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from operator import itemgetter\n",
    "import torch.optim as optim\n",
    "\n",
    "from functools import partial, reduce\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = train_data[\"processed_text\"].str.len().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.loc[sorted_idx]\n",
    "y_train = train_data[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[\"processed_text\"].values\n",
    "X_test = test_data[\"processed_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 6036),\n",
       " ('я', 2924),\n",
       " ('это', 2701),\n",
       " ('всё', 1623),\n",
       " ('весь', 1424),\n",
       " ('свой', 1224),\n",
       " ('человек', 903),\n",
       " ('тварь', 902),\n",
       " ('ещё', 800),\n",
       " ('сука', 781),\n",
       " ('хуй', 703),\n",
       " ('твой', 680),\n",
       " ('мочь', 658),\n",
       " ('d', 623),\n",
       " ('сосать', 607),\n",
       " ('ребёнок', 590),\n",
       " ('нет', 583),\n",
       " ('мразь', 566),\n",
       " ('пидор', 550),\n",
       " ('хотеть', 525),\n",
       " ('жопа', 525),\n",
       " ('хороший', 511),\n",
       " ('урод', 509),\n",
       " ('который', 508),\n",
       " ('народ', 468),\n",
       " ('просто', 465),\n",
       " ('год', 455),\n",
       " ('наш', 446),\n",
       " ('блядь', 446),\n",
       " ('россия', 429),\n",
       " ('нужно', 425),\n",
       " ('расстрелять', 415),\n",
       " ('дать', 394),\n",
       " ('жить', 392),\n",
       " ('пусть', 390),\n",
       " ('знать', 389),\n",
       " ('рука', 387),\n",
       " ('идти', 352),\n",
       " ('путин', 347),\n",
       " ('ваш', 344),\n",
       " ('жизнь', 342),\n",
       " ('страна', 337),\n",
       " ('рот', 326),\n",
       " ('сказать', 320),\n",
       " ('очень', 320),\n",
       " ('большой', 317),\n",
       " ('кол', 300),\n",
       " ('говорить', 298),\n",
       " ('день', 288),\n",
       " ('убить', 287),\n",
       " ('место', 286),\n",
       " ('делать', 280),\n",
       " ('ублюдок', 265),\n",
       " ('б', 262),\n",
       " ('слово', 254),\n",
       " ('деньга', 254),\n",
       " ('её', 253),\n",
       " ('думать', 241),\n",
       " ('вообще', 240),\n",
       " ('писать', 238),\n",
       " ('сразу', 233),\n",
       " ('дебил', 232),\n",
       " ('сделать', 230),\n",
       " ('смотреть', 227),\n",
       " ('сидеть', 226),\n",
       " ('русский', 225),\n",
       " ('время', 224),\n",
       " ('самый', 222),\n",
       " ('давать', 221),\n",
       " ('голова', 219),\n",
       " ('сколько', 215),\n",
       " ('пидорас', 209),\n",
       " ('бог', 208),\n",
       " ('яйцо', 208),\n",
       " ('власть', 208),\n",
       " ('трахать', 207),\n",
       " ('убивать', 207),\n",
       " ('мужик', 204),\n",
       " ('пидарас', 203),\n",
       " ('пока', 203),\n",
       " ('пизда', 201),\n",
       " ('почему', 201),\n",
       " ('посадить', 200),\n",
       " ('каждый', 200),\n",
       " ('нужный', 198),\n",
       " ('друг', 197),\n",
       " ('нахуй', 194),\n",
       " ('нога', 192),\n",
       " ('мама', 190),\n",
       " ('должный', 187),\n",
       " ('пора', 183),\n",
       " ('стрелять', 182),\n",
       " ('сдохнуть', 182),\n",
       " ('видеть', 180),\n",
       " ('ебал', 177),\n",
       " ('написать', 177),\n",
       " ('дом', 176),\n",
       " ('пидар', 175),\n",
       " ('очко', 174),\n",
       " ('любить', 173),\n",
       " ('женщина', 172),\n",
       " ('шлюха', 169),\n",
       " ('дело', 168),\n",
       " ('спасибо', 167),\n",
       " ('бить', 167),\n",
       " ('стать', 166),\n",
       " ('работать', 165),\n",
       " ('ёбаный', 164),\n",
       " ('член', 162),\n",
       " ('давно', 161),\n",
       " ('трахнуть', 160),\n",
       " ('первый', 159),\n",
       " ('собака', 158),\n",
       " ('вместе', 157),\n",
       " ('мать', 155),\n",
       " ('ебать', 153),\n",
       " ('мало', 152),\n",
       " ('никто', 151),\n",
       " ('повесить', 150),\n",
       " ('тупой', 149),\n",
       " ('молодец', 148),\n",
       " ('родитель', 147),\n",
       " ('здоровье', 145),\n",
       " ('земля', 145),\n",
       " ('пойти', 145),\n",
       " ('мозг', 145),\n",
       " ('сволочь', 143),\n",
       " ('понять', 142),\n",
       " ('взять', 141),\n",
       " ('отсосать', 140),\n",
       " ('козёл', 140),\n",
       " ('х', 140),\n",
       " ('посмотреть', 135),\n",
       " ('найти', 135),\n",
       " ('суд', 133),\n",
       " ('кончить', 132),\n",
       " ('ждать', 132),\n",
       " ('иметь', 131),\n",
       " ('мир', 131),\n",
       " ('продажный', 130),\n",
       " ('правильно', 129),\n",
       " ('добрый', 128),\n",
       " ('работа', 128),\n",
       " ('класс', 127),\n",
       " ('фото', 127),\n",
       " ('путинский', 123),\n",
       " ('семья', 123),\n",
       " ('скоро', 122),\n",
       " ('украина', 122),\n",
       " ('чмо', 120),\n",
       " ('бля', 120),\n",
       " ('маленький', 120),\n",
       " ('ад', 119),\n",
       " ('красивый', 119),\n",
       " ('придурок', 119),\n",
       " ('полный', 118),\n",
       " ('гнида', 118),\n",
       " ('пиздить', 117),\n",
       " ('глаз', 116),\n",
       " ('2', 116),\n",
       " ('хер', 116),\n",
       " ('гореть', 115),\n",
       " ('вопрос', 115),\n",
       " ('начать', 113),\n",
       " ('получить', 113),\n",
       " ('старый', 112),\n",
       " ('нибыть', 112),\n",
       " ('остаться', 112),\n",
       " ('fr', 111),\n",
       " ('хотеться', 111),\n",
       " ('лизать', 111),\n",
       " ('оторвать', 110),\n",
       " ('точно', 110),\n",
       " ('гандон', 109),\n",
       " ('дура', 109),\n",
       " ('засунуть', 109),\n",
       " ('растрелять', 108),\n",
       " ('идиот', 108),\n",
       " ('ответить', 107),\n",
       " ('дрочить', 107),\n",
       " ('простой', 107),\n",
       " ('смерть', 107),\n",
       " ('понимать', 107),\n",
       " ('девочка', 105),\n",
       " ('язык', 104),\n",
       " ('душа', 104),\n",
       " ('баба', 103),\n",
       " ('говно', 103),\n",
       " ('право', 102),\n",
       " ('поставить', 101),\n",
       " ('группа', 101),\n",
       " ('стенка', 100),\n",
       " ('видно', 100),\n",
       " ('че', 100),\n",
       " ('всякий', 100),\n",
       " ('животное', 99),\n",
       " ('прийти', 99),\n",
       " ('значит', 98),\n",
       " ('закон', 98),\n",
       " ('хуйня', 97),\n",
       " ('судить', 97),\n",
       " ('правда', 97),\n",
       " ('отрезать', 96),\n",
       " ('вонючий', 96),\n",
       " ('хозяин', 95),\n",
       " ('мочить', 95),\n",
       " ('з', 95),\n",
       " ('ходить', 94),\n",
       " ('далёкий', 94),\n",
       " ('морда', 93),\n",
       " ('расстреливать', 92),\n",
       " ('казнить', 92),\n",
       " ('последний', 92),\n",
       " ('хотя', 92),\n",
       " ('отрубить', 91),\n",
       " ('1', 91),\n",
       " ('площадь', 91),\n",
       " ('счастие', 91),\n",
       " ('операция', 91),\n",
       " ('сегодня', 90),\n",
       " ('помнить', 90),\n",
       " ('бояться', 90),\n",
       " ('конченый', 90),\n",
       " ('ч', 90),\n",
       " ('проститутка', 90),\n",
       " ('наверное', 89),\n",
       " ('стоить', 89),\n",
       " ('плохой', 89),\n",
       " ('жена', 88),\n",
       " ('настоящий', 88),\n",
       " ('любой', 87),\n",
       " ('ести', 87),\n",
       " ('пиздец', 87),\n",
       " ('машина', 86),\n",
       " ('сжечь', 86),\n",
       " ('муж', 86),\n",
       " ('подвесить', 86),\n",
       " ('город', 85),\n",
       " ('уничтожать', 85),\n",
       " ('батька', 85),\n",
       " ('цена', 84),\n",
       " ('читать', 84),\n",
       " ('брать', 84),\n",
       " ('против', 83),\n",
       " ('вода', 83),\n",
       " ('родина', 83),\n",
       " ('нормальный', 83),\n",
       " ('кастрировать', 82),\n",
       " ('дурак', 82),\n",
       " ('башка', 82),\n",
       " ('новый', 82),\n",
       " ('туда', 82),\n",
       " ('президент', 82),\n",
       " ('конец', 80),\n",
       " ('дорога', 79),\n",
       " ('пенсия', 79),\n",
       " ('5', 79),\n",
       " ('ссср', 79),\n",
       " ('выйти', 78),\n",
       " ('мужчина', 78),\n",
       " ('лицо', 78),\n",
       " ('также', 78),\n",
       " ('хватить', 78),\n",
       " ('номер', 77),\n",
       " ('дерьмо', 77),\n",
       " ('жаль', 77),\n",
       " ('хрен', 77),\n",
       " ('сажать', 76),\n",
       " ('второй', 76),\n",
       " ('война', 76),\n",
       " ('любовь', 76),\n",
       " ('кормить', 76),\n",
       " ('показать', 76),\n",
       " ('помочь', 75),\n",
       " ('отдать', 75),\n",
       " ('купить', 75),\n",
       " ('ранний', 75),\n",
       " ('рожа', 74),\n",
       " ('рак', 74),\n",
       " ('господь', 74),\n",
       " ('поганый', 74),\n",
       " ('карта', 74),\n",
       " ('ебут', 73),\n",
       " ('фашист', 73),\n",
       " ('хватать', 73),\n",
       " ('улица', 73),\n",
       " ('предатель', 72),\n",
       " ('расстрел', 72),\n",
       " ('давить', 72),\n",
       " ('интересно', 71),\n",
       " ('привет', 71),\n",
       " ('й', 71),\n",
       " ('далее', 71),\n",
       " ('ru', 71),\n",
       " ('нравиться', 70),\n",
       " ('красота', 70),\n",
       " ('проблема', 70),\n",
       " ('забыть', 70),\n",
       " ('задница', 70),\n",
       " ('вор', 70),\n",
       " ('оставить', 69),\n",
       " ('жалко', 69),\n",
       " ('ой', 69),\n",
       " ('долбоести', 69),\n",
       " ('гнать', 69),\n",
       " ('продать', 69),\n",
       " ('рождение', 69),\n",
       " ('здравствуйте', 69),\n",
       " ('8oi', 69),\n",
       " ('рядом', 69),\n",
       " ('вешать', 69),\n",
       " ('верить', 69),\n",
       " ('согласный', 68),\n",
       " ('отец', 68),\n",
       " ('больной', 68),\n",
       " ('москва', 68),\n",
       " ('сын', 67),\n",
       " ('равно', 67),\n",
       " ('пожалуйста', 67),\n",
       " ('быдло', 67),\n",
       " ('платить', 67),\n",
       " ('3', 66),\n",
       " ('супер', 66),\n",
       " ('ребята', 66),\n",
       " ('отстреливать', 66),\n",
       " ('сталин', 66),\n",
       " ('чужой', 66),\n",
       " ('бедный', 66),\n",
       " ('шея', 65),\n",
       " ('уметь', 65),\n",
       " ('носить', 65),\n",
       " ('падло', 65),\n",
       " ('считать', 64),\n",
       " ('долго', 64),\n",
       " ('искать', 64),\n",
       " ('родный', 64),\n",
       " ('остальной', 64),\n",
       " ('привязать', 63),\n",
       " ('убийца', 63),\n",
       " ('просить', 63),\n",
       " ('ум', 63),\n",
       " ('лес', 62),\n",
       " ('девушка', 62),\n",
       " ('увидеть', 62),\n",
       " ('нечего', 62),\n",
       " ('ебануть', 61),\n",
       " ('10', 61),\n",
       " ('свинья', 61),\n",
       " ('молодой', 61),\n",
       " ('хохол', 61),\n",
       " ('оба', 60),\n",
       " ('враг', 60),\n",
       " ('никакой', 60),\n",
       " ('умный', 59),\n",
       " ('главное', 59),\n",
       " ('выебать', 59),\n",
       " ('врач', 59),\n",
       " ('столько', 59),\n",
       " ('любимый', 59),\n",
       " ('сила', 59),\n",
       " ('дед', 59),\n",
       " ('ok', 59),\n",
       " ('хабаровск', 58),\n",
       " ('4', 58),\n",
       " ('поймать', 58),\n",
       " ('зарплата', 58),\n",
       " ('следствие', 58),\n",
       " ('автор', 58),\n",
       " ('тема', 58),\n",
       " ('подобный', 58),\n",
       " ('ха', 58),\n",
       " ('оо', 58),\n",
       " ('вдуть', 57),\n",
       " ('месяц', 57),\n",
       " ('случай', 57),\n",
       " ('жрать', 57),\n",
       " ('попка', 57),\n",
       " ('родиться', 57),\n",
       " ('п', 57),\n",
       " ('лукашенко', 57),\n",
       " ('иначе', 57),\n",
       " ('школа', 57),\n",
       " ('руб', 57),\n",
       " ('живой', 56),\n",
       " ('папа', 56),\n",
       " ('красный', 56),\n",
       " ('гад', 56),\n",
       " ('тюрьма', 56),\n",
       " ('герой', 56),\n",
       " ('парень', 55),\n",
       " ('ужас', 55),\n",
       " ('дырявый', 55),\n",
       " ('подонок', 55),\n",
       " ('забрать', 55),\n",
       " ('пиндос', 55),\n",
       " ('мудак', 55),\n",
       " ('20', 55),\n",
       " ('держать', 55),\n",
       " ('долбоеб', 55),\n",
       " ('слушать', 55),\n",
       " ('мнение', 55),\n",
       " ('государство', 55),\n",
       " ('смочь', 55),\n",
       " ('секс', 54),\n",
       " ('ебало', 54),\n",
       " ('наказать', 54),\n",
       " ('прям', 54),\n",
       " ('h', 54),\n",
       " ('бывать', 54),\n",
       " ('дума', 54),\n",
       " ('уничтожить', 54),\n",
       " ('позор', 54),\n",
       " ('хуйло', 53),\n",
       " ('готовый', 53),\n",
       " ('стоять', 53),\n",
       " ('пьяный', 53),\n",
       " ('закрыть', 53),\n",
       " ('игра', 53),\n",
       " ('история', 53),\n",
       " ('начинать', 53),\n",
       " ('кроме', 53),\n",
       " ('европа', 53),\n",
       " ('чёрт', 52),\n",
       " ('очередной', 52),\n",
       " ('послать', 52),\n",
       " ('больший', 52),\n",
       " ('лезть', 52),\n",
       " ('начало', 52),\n",
       " ('видео', 52),\n",
       " ('телефон', 51),\n",
       " ('сраный', 51),\n",
       " ('сначала', 51),\n",
       " ('снимать', 51),\n",
       " ('порвать', 51),\n",
       " ('пристрелить', 51),\n",
       " ('мусор', 51),\n",
       " ('алкаш', 51),\n",
       " ('желать', 51),\n",
       " ('тысяча', 51),\n",
       " ('вид', 51),\n",
       " ('охуесть', 50),\n",
       " ('надеяться', 50),\n",
       " ('сучка', 50),\n",
       " ('откуда', 50),\n",
       " ('открыть', 50),\n",
       " ('ставить', 50),\n",
       " ('вести', 50),\n",
       " ('уйти', 50),\n",
       " ('родить', 50),\n",
       " ('поэтому', 50),\n",
       " ('сторона', 50),\n",
       " ('вернуть', 50),\n",
       " ('пол', 50),\n",
       " ('попробовать', 49),\n",
       " ('срак', 49),\n",
       " ('ебучий', 49),\n",
       " ('пускай', 49),\n",
       " ('кровь', 49),\n",
       " ('собирать', 49),\n",
       " ('бабка', 49),\n",
       " ('свет', 49),\n",
       " ('нахер', 48),\n",
       " ('выродок', 48),\n",
       " ('поп', 48),\n",
       " ('трахаться', 48),\n",
       " ('бред', 48),\n",
       " ('страшный', 48),\n",
       " ('утро', 48),\n",
       " ('лично', 48),\n",
       " ('называть', 48),\n",
       " ('петух', 48),\n",
       " ('узнать', 48),\n",
       " ('пройти', 48),\n",
       " ('учить', 48),\n",
       " ('счёт', 48),\n",
       " ('100', 48),\n",
       " ('назвать', 48),\n",
       " ('помощь', 48),\n",
       " ('набить', 47),\n",
       " ('дочь', 47),\n",
       " ('простить', 47),\n",
       " ('овца', 47),\n",
       " ('гондон', 47),\n",
       " ('поднять', 47),\n",
       " ('колено', 47),\n",
       " ('именно', 47),\n",
       " ('белый', 47),\n",
       " ('стыдно', 47),\n",
       " ('отвечать', 47),\n",
       " ('брат', 47),\n",
       " ('пить', 47),\n",
       " ('баран', 47),\n",
       " ('палец', 47),\n",
       " ('лишь', 47),\n",
       " ('решить', 47),\n",
       " ('вещь', 47),\n",
       " ('сиська', 46),\n",
       " ('мент', 46),\n",
       " ('выести', 46),\n",
       " ('шакал', 46),\n",
       " ('зад', 46),\n",
       " ('показывать', 46),\n",
       " ('разный', 46),\n",
       " ('имя', 46),\n",
       " ('час', 46),\n",
       " ('получать', 46),\n",
       " ('быстро', 45),\n",
       " ('уважать', 45),\n",
       " ('депутат', 45),\n",
       " ('выблядок', 45),\n",
       " ('действительно', 45),\n",
       " ('везде', 45),\n",
       " ('заниматься', 45),\n",
       " ('порядок', 45),\n",
       " ('крым', 45),\n",
       " ('придумать', 45),\n",
       " ('зато', 45),\n",
       " ('россиянин', 45),\n",
       " ('отношение', 45),\n",
       " ('нести', 45),\n",
       " ('бабушка', 44),\n",
       " ('август', 44),\n",
       " ('мразот', 44),\n",
       " ('уебок', 44),\n",
       " ('китай', 44),\n",
       " ('мамаша', 44),\n",
       " ('кусок', 44),\n",
       " ('р', 44),\n",
       " ('граница', 44),\n",
       " ('близкий', 44),\n",
       " ('мальчик', 43),\n",
       " ('дорогой', 43),\n",
       " ('удовольствие', 43),\n",
       " ('понятно', 43),\n",
       " ('ехать', 43),\n",
       " ('ненавидеть', 43),\n",
       " ('оно', 43),\n",
       " ('советский', 43),\n",
       " ('скот', 43),\n",
       " ('маска', 43),\n",
       " ('правительство', 43),\n",
       " ('https', 43),\n",
       " ('плохо', 42),\n",
       " ('пиздабол', 42),\n",
       " ('издеваться', 42),\n",
       " ('великий', 42),\n",
       " ('комментарий', 42),\n",
       " ('зона', 42),\n",
       " ('отправить', 42),\n",
       " ('крыса', 42),\n",
       " ('вдв', 42),\n",
       " ('трус', 42),\n",
       " ('ночь', 42),\n",
       " ('дочка', 42),\n",
       " ('пососать', 41),\n",
       " ('дерево', 41),\n",
       " ('дырка', 41),\n",
       " ('огромный', 41),\n",
       " ('0', 41),\n",
       " ('покупать', 41),\n",
       " ('слава', 41),\n",
       " ('15', 41),\n",
       " ('сломать', 40),\n",
       " ('деревня', 40),\n",
       " ('плешивый', 40),\n",
       " ('падаль', 40),\n",
       " ('насосать', 40),\n",
       " ('удача', 40),\n",
       " ('нах', 40),\n",
       " ('повадно', 40),\n",
       " ('педофил', 40),\n",
       " ('очередь', 40),\n",
       " ('особенно', 40),\n",
       " ('заебали', 40),\n",
       " ('ухо', 40),\n",
       " ('подсказать', 40),\n",
       " ('рубль', 40),\n",
       " ('еб', 40),\n",
       " ('учиться', 40),\n",
       " ('приехать', 40),\n",
       " ('пытаться', 40),\n",
       " ('либо', 40),\n",
       " ('похожий', 39),\n",
       " ('поздравлять', 39),\n",
       " ('гей', 39),\n",
       " ('валить', 39),\n",
       " ('чурка', 39),\n",
       " ('кот', 39),\n",
       " ('малыш', 39),\n",
       " ('фу', 39),\n",
       " ('бросить', 39),\n",
       " ('разорвать', 39),\n",
       " ('вчера', 39),\n",
       " ('спасти', 39),\n",
       " ('народный', 38),\n",
       " ('играть', 38),\n",
       " ('чей', 38),\n",
       " ('пиздеть', 38),\n",
       " ('хулить', 38),\n",
       " ('видать', 38),\n",
       " ('защищать', 38),\n",
       " ('обезьяна', 38),\n",
       " ('убрать', 38),\n",
       " ('проклятый', 38),\n",
       " ('блядина', 38),\n",
       " ('черта', 38),\n",
       " ('воровать', 38),\n",
       " ('голосовать', 38),\n",
       " ('армия', 38),\n",
       " ('сайт', 38),\n",
       " ('завод', 38),\n",
       " ('виноватый', 38),\n",
       " ('расти', 38),\n",
       " ('сердце', 38),\n",
       " ('положить', 38),\n",
       " ('грязный', 38),\n",
       " ('надоесть', 37),\n",
       " ('классный', 37),\n",
       " ('гнилой', 37),\n",
       " ('рожать', 37),\n",
       " ('век', 37),\n",
       " ('потерять', 37),\n",
       " ('безмозглый', 37),\n",
       " ('миллион', 37),\n",
       " ('ебета', 37),\n",
       " ('внимание', 37),\n",
       " ('бабло', 37),\n",
       " ('г', 37),\n",
       " ('америка', 37),\n",
       " ('лайк', 37),\n",
       " ('вернуться', 37),\n",
       " ('донбасс', 37),\n",
       " ('сюда', 37),\n",
       " ('30', 37),\n",
       " ('лежать', 37),\n",
       " ('пост', 37),\n",
       " ('ебальник', 36),\n",
       " ('еврей', 36),\n",
       " ('блин', 36),\n",
       " ('ответ', 36),\n",
       " ('растреливать', 36),\n",
       " ('хлеб', 36),\n",
       " ('наверно', 36),\n",
       " ('пара', 36),\n",
       " ('прямо', 36),\n",
       " ('причём', 36),\n",
       " ('выходить', 36),\n",
       " ('попасть', 36),\n",
       " ('детство', 36),\n",
       " ('сей', 36),\n",
       " ('некоторый', 36),\n",
       " ('творить', 35),\n",
       " ('ох', 35),\n",
       " ('нация', 35),\n",
       " ('6', 35),\n",
       " ('милый', 35),\n",
       " ('совесть', 35),\n",
       " ('клоун', 35),\n",
       " ('гора', 35),\n",
       " ('лёгкий', 35),\n",
       " ('дешёвый', 35),\n",
       " ('подумать', 35),\n",
       " ('беларусь', 35),\n",
       " ('шалавый', 35),\n",
       " ('тип', 35),\n",
       " ('белорус', 35),\n",
       " ('вырасти', 35),\n",
       " ('назад', 35),\n",
       " ('море', 34),\n",
       " ('возраст', 34),\n",
       " ('служить', 34),\n",
       " ('сосед', 34),\n",
       " ('найтись', 34),\n",
       " ('казнь', 34),\n",
       " ('видимо', 34),\n",
       " ('праздник', 34),\n",
       " ('неделя', 34),\n",
       " ('жид', 34),\n",
       " ('вроде', 34),\n",
       " ('админ', 34),\n",
       " ('прекрасный', 34),\n",
       " ('образование', 34),\n",
       " ('казаться', 34),\n",
       " ('7', 34),\n",
       " ('стена', 34),\n",
       " ('слышать', 34),\n",
       " ('богатый', 34),\n",
       " ('внук', 34),\n",
       " ('o', 33),\n",
       " ('противно', 33),\n",
       " ('живьём', 33),\n",
       " ('полицай', 33),\n",
       " ('вырвать', 33),\n",
       " ('размер', 33),\n",
       " ('принимать', 33),\n",
       " ('получиться', 33),\n",
       " ('помогать', 33),\n",
       " ('красиво', 33),\n",
       " ('лоб', 33),\n",
       " ('следующий', 33),\n",
       " ('красавица', 33),\n",
       " ('часть', 33),\n",
       " ('фильм', 33),\n",
       " ('н', 33),\n",
       " ('магазин', 33),\n",
       " ('пустить', 33),\n",
       " ('никак', 33),\n",
       " ('камера', 33),\n",
       " ('оказаться', 33),\n",
       " ('принять', 33),\n",
       " ('здоровый', 33),\n",
       " ('сша', 33),\n",
       " ('развалить', 33),\n",
       " ('встать', 33),\n",
       " ('взрослый', 33),\n",
       " ('белоруссия', 33),\n",
       " ('profile', 33),\n",
       " ('сбербанк', 33),\n",
       " ('пися', 32),\n",
       " ('мерзкий', 32),\n",
       " ('быстрый', 32),\n",
       " ('скотина', 32),\n",
       " ('похоже', 32),\n",
       " ('реально', 32),\n",
       " ('срок', 32),\n",
       " ('память', 32),\n",
       " ('детский', 32),\n",
       " ('рыжий', 32),\n",
       " ('святой', 32),\n",
       " ('спросить', 32),\n",
       " ('прислать', 32),\n",
       " ('снять', 32),\n",
       " ('интересный', 32),\n",
       " ('нада', 32),\n",
       " ('около', 32),\n",
       " ('рыло', 32),\n",
       " ('тело', 32),\n",
       " ('полиция', 32),\n",
       " ('третий', 32),\n",
       " ('хует', 32),\n",
       " ('позорный', 32),\n",
       " ('митинг', 32),\n",
       " ('квартира', 32),\n",
       " ('ребёночек', 32),\n",
       " ('область', 32),\n",
       " ('число', 32),\n",
       " ('трогать', 32),\n",
       " ('заставить', 32),\n",
       " ('гражданин', 32),\n",
       " ('погибнуть', 32),\n",
       " ('помойка', 32),\n",
       " ('аж', 32),\n",
       " ('терпеть', 32),\n",
       " ('запад', 32),\n",
       " ('ездить', 31),\n",
       " ('слишком', 31),\n",
       " ('молчать', 31),\n",
       " ('отсасывать', 31),\n",
       " ('угол', 31),\n",
       " ('приезжать', 31),\n",
       " ('продавать', 31),\n",
       " ('закопать', 31),\n",
       " ('золотой', 31),\n",
       " ('поддерживать', 31),\n",
       " ('скорее', 31),\n",
       " ('боль', 31),\n",
       " ('черножопый', 31),\n",
       " ('рассказать', 31),\n",
       " ('чёрный', 31),\n",
       " ('фотка', 31),\n",
       " ('вечно', 31),\n",
       " ('перестать', 31),\n",
       " ('минута', 31),\n",
       " ('бегать', 31),\n",
       " ('нету', 31),\n",
       " ('тупорылый', 31),\n",
       " ('масло', 31),\n",
       " ('пример', 31),\n",
       " ('американский', 31),\n",
       " ('мск', 31),\n",
       " ('уебище', 30),\n",
       " ('пи', 30),\n",
       " ('звонить', 30),\n",
       " ('российский', 30),\n",
       " ('свобода', 30),\n",
       " ('завтра', 30),\n",
       " ('русь', 30),\n",
       " ('лечить', 30),\n",
       " ('добро', 30),\n",
       " ('интернет', 30),\n",
       " ('разбить', 30),\n",
       " ('неужели', 30),\n",
       " ('научиться', 30),\n",
       " ('корень', 30),\n",
       " ('опустить', 30),\n",
       " ('стадо', 30),\n",
       " ('заработать', 30),\n",
       " ('кремль', 30),\n",
       " ('добавить', 30),\n",
       " ('домой', 30),\n",
       " ('тролль', 30),\n",
       " ('похуй', 30),\n",
       " ('сумма', 30),\n",
       " ('путь', 30),\n",
       " ('выполнить', 30),\n",
       " ('получатель', 30),\n",
       " ('точка', 29),\n",
       " ('природа', 29),\n",
       " ('документ', 29),\n",
       " ('позвонить', 29),\n",
       " ('бешеный', 29),\n",
       " ('счастливый', 29),\n",
       " ('песня', 29),\n",
       " ('еблан', 29),\n",
       " ('хрень', 29),\n",
       " ('долбаести', 29),\n",
       " ('толк', 29),\n",
       " ('бандеровский', 29),\n",
       " ('выставлять', 29),\n",
       " ('смертный', 29),\n",
       " ('нормально', 29),\n",
       " ('прийтись', 29),\n",
       " ('прилюдно', 29),\n",
       " ('зажраться', 29),\n",
       " ('лето', 29),\n",
       " ('сильный', 29),\n",
       " ('ручка', 29),\n",
       " ('пёс', 29),\n",
       " ('бесплатно', 29),\n",
       " ('достать', 29),\n",
       " ('собрать', 29),\n",
       " ('40', 29),\n",
       " ('общаться', 29),\n",
       " ('пацан', 29),\n",
       " ('пасть', 29),\n",
       " ('успеть', 29),\n",
       " ('несколько', 29),\n",
       " ('уважение', 28),\n",
       " ('короче', 28),\n",
       " ('рвать', 28),\n",
       " ('ебанутаить', 28),\n",
       " ('классно', 28),\n",
       " ('понравиться', 28),\n",
       " ('сперма', 28),\n",
       " ('провокатор', 28),\n",
       " ('здохнуть', 28),\n",
       " ('чудо', 28),\n",
       " ('общество', 28),\n",
       " ('фамилия', 28),\n",
       " ('район', 28),\n",
       " ('статья', 28),\n",
       " ('херня', 28),\n",
       " ('уровень', 28),\n",
       " ('пенсионер', 28),\n",
       " ('целый', 28),\n",
       " ('совет', 28),\n",
       " ('корова', 28),\n",
       " ('резать', 28),\n",
       " ('ах', 28),\n",
       " ('родственник', 28),\n",
       " ('курс', 28),\n",
       " ('род', 28),\n",
       " ('бывший', 28),\n",
       " ('шкура', 28),\n",
       " ('форма', 28),\n",
       " ('плакать', 27),\n",
       " ('сутки', 27),\n",
       " ('прибить', 27),\n",
       " ('толпа', 27),\n",
       " ('забить', 27),\n",
       " ('ебу', 27),\n",
       " ('называться', 27),\n",
       " ('ага', 27),\n",
       " ('наслать', 27),\n",
       " ('уходить', 27),\n",
       " ('происходить', 27),\n",
       " ('турок', 27),\n",
       " ('негр', 27),\n",
       " ('ебанный', 27),\n",
       " ('страшно', 27),\n",
       " ('поступок', 27),\n",
       " ('25', 27),\n",
       " ('личный', 27),\n",
       " ('информация', 27),\n",
       " ('выше', 27),\n",
       " ('радость', 27),\n",
       " ('скинуть', 27),\n",
       " ('м', 27),\n",
       " ('слышь', 27),\n",
       " ('сеть', 27),\n",
       " ('90', 27),\n",
       " ('штраф', 27),\n",
       " ('онлайн', 27),\n",
       " ('беда', 26),\n",
       " ('садить', 26),\n",
       " ('жидовский', 26),\n",
       " ('фашистский', 26),\n",
       " ('хохлов', 26),\n",
       " ('соска', 26),\n",
       " ('светлый', 26),\n",
       " ('горло', 26),\n",
       " ('выкинуть', 26),\n",
       " ('адвокат', 26),\n",
       " ('папаша', 26),\n",
       " ('солнышко', 26),\n",
       " ('союз', 26),\n",
       " ('актёр', 26),\n",
       " ('p', 26),\n",
       " ('мешать', 26),\n",
       " ('50', 26),\n",
       " ('крепкий', 26),\n",
       " ('трусливый', 26),\n",
       " ('воевать', 26),\n",
       " ('ефрем', 26),\n",
       " ('единый', 26),\n",
       " ('книга', 26),\n",
       " ('майдан', 26),\n",
       " ('мясо', 26),\n",
       " ('петь', 26),\n",
       " ('запомнить', 26),\n",
       " ('высокий', 26),\n",
       " ('состояние', 26),\n",
       " ('желание', 26),\n",
       " ('моральный', 26),\n",
       " ('чиновник', 26),\n",
       " ('кстати', 26),\n",
       " ('чек', 26),\n",
       " ('старик', 26),\n",
       " ('постоянно', 26),\n",
       " ('обязательно', 26),\n",
       " ('кричать', 26),\n",
       " ('album', 26),\n",
       " ('отсохнуть', 25),\n",
       " ('осиновый', 25),\n",
       " ('сказка', 25),\n",
       " ('успокоиться', 25),\n",
       " ('долбоебова', 25),\n",
       " ('выглядеть', 25),\n",
       " ('вера', 25),\n",
       " ('гроб', 25),\n",
       " ('мечтать', 25),\n",
       " ('предлагать', 25),\n",
       " ('спрашивать', 25),\n",
       " ('вспомнить', 25),\n",
       " ('чистый', 25),\n",
       " ('будущее', 25),\n",
       " ('немного', 25),\n",
       " ('верный', 25),\n",
       " ('л', 25),\n",
       " ('военный', 25),\n",
       " ('утопить', 25),\n",
       " ('таки', 25),\n",
       " ('соль', 25),\n",
       " ('приятно', 25),\n",
       " ('находиться', 25),\n",
       " ('извинить', 25),\n",
       " ('стол', 25),\n",
       " ('поход', 25),\n",
       " ('процент', 25),\n",
       " ('погон', 25),\n",
       " ('ошибка', 25),\n",
       " ('рецепт', 25),\n",
       " ('сообщение', 25),\n",
       " ('зря', 25),\n",
       " ('устроить', 25),\n",
       " ('поднимать', 25),\n",
       " ('получаться', 25),\n",
       " ('болеть', 25),\n",
       " ('спать', 25),\n",
       " ('факт', 25),\n",
       " ('раб', 25),\n",
       " ('копейка', 25),\n",
       " ('цель', 25),\n",
       " ('скорый', 25),\n",
       " ('украинский', 25),\n",
       " ('бб', 25),\n",
       " ('ур', 25),\n",
       " ('глотка', 24),\n",
       " ('спасать', 24),\n",
       " ('китаец', 24),\n",
       " ('гавный', 24),\n",
       " ('нож', 24),\n",
       " ('заебал', 24),\n",
       " ('пожизненно', 24),\n",
       " ('пиздюля', 24),\n",
       " ('чучело', 24),\n",
       " ('ебиться', 24),\n",
       " ('хуесос', 24),\n",
       " ('сто', 24),\n",
       " ('асфальт', 24),\n",
       " ('флаг', 24),\n",
       " ('выборы', 24),\n",
       " ('драть', 24),\n",
       " ('долгий', 24),\n",
       " ('фургал', 24),\n",
       " ('судья', 24),\n",
       " ('многие', 24),\n",
       " ('появиться', 24),\n",
       " ('водитель', 24),\n",
       " ('звание', 24),\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(reduce(lambda x, y: x + y, X_train))\n",
    "c.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26735"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dist = pd.Series(c).sort_values(ascending=False).head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_dist.plot.area(figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsums = (word_dist / c.total()).cumsum().reset_index(name=\"cumsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cumsums.query(\"cumsum <= 0.50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10479\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(cumsums.query(\"cumsum <= 0.9\")) \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<UNK>\"] + cumsums[\"index\"].iloc[0:vocab_size].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 300\n",
    "NUM_CLASSES = 4\n",
    "NUM_EPOCHS = 5\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 5e-3\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstmForClassification(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ff = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "        self.embedding = None\n",
    "        self._word2idx = None\n",
    "        self._embed = None\n",
    "\n",
    "    def forward(self, texts):\n",
    "        if self._embed is None:\n",
    "            raise NotImplementedError(\"You forgot to init embeddings.\")\n",
    "\n",
    "        embeddings = self._embed(texts)\n",
    "        embeddings = self._prepare_embeddings(embeddings)\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(embeddings)\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "\n",
    "        return self.ff(hidden)\n",
    "\n",
    "    def init_embeddings(self, learn_embeddings, **kwargs):\n",
    "        if learn_embeddings:\n",
    "            self._word2idx = kwargs[\"word2idx\"]\n",
    "\n",
    "            embedding_matrix = self._init_embedding_matrix()\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "            self._embed = self._embed_self\n",
    "        else:\n",
    "            self._embed = self._embed_navec\n",
    "    \n",
    "    def _init_embedding_matrix(self):\n",
    "        vocab_size = len(self._word2idx)\n",
    "        init_matrix = np.random.rand(vocab_size, self.emb_size)\n",
    "\n",
    "        for word, idx in self._word2idx.items():\n",
    "            navec_emb = navec.get(word)\n",
    "            if navec_emb is not None:\n",
    "                init_matrix[idx] = navec_emb\n",
    "        \n",
    "        return torch.from_numpy(init_matrix).to(torch.float32)\n",
    "\n",
    "    def _embed_self(self, texts: list[list[str]]) -> list[torch.Tensor]:\n",
    "        ids = list(map(self._convert_words2idx, texts))\n",
    "        embeddings = list(map(self.embedding, ids))\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _convert_words2idx(self, text: list[str]) -> list[int]:\n",
    "        encoded = [self._word2idx.get(w, self._word2idx[\"<UNK>\"]) for w in text]\n",
    "        return torch.tensor(encoded, dtype=torch.int)\n",
    "\n",
    "    @staticmethod\n",
    "    def _embed_navec(texts: list[list[str]]) -> list[torch.Tensor]:\n",
    "        embeddings = list(map(partial(vectorize, mean=False), texts))\n",
    "        embeddings = list(map(torch.from_numpy, embeddings))\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_embeddings(embeddings: list[torch.Tensor]):\n",
    "        lengths = list(map(len, embeddings))\n",
    "\n",
    "        embeddings = nn.utils.rnn.pad_sequence(embeddings, batch_first=True)\n",
    "        embeddings = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeddings, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        return embeddings.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    texts = list(map(itemgetter(0), batch))\n",
    "    labels = list(map(itemgetter(1), batch))\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LstmDataset(X_train, y_train)\n",
    "test_dataset = LstmDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLstmForClassification(\n",
       "  (lstm): LSTM(300, 300, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (ff): Linear(in_features=600, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = BiLstmForClassification(EMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES)\n",
    "lstm.init_embeddings(learn_embeddings=False, word2idx=word2idx)\n",
    "lstm.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksiej/miniconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    lstm.train()\n",
    "\n",
    "    losses = []\n",
    "    for texts, labels in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = lstm(texts)\n",
    "\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_epoch():\n",
    "    lstm.eval()\n",
    "\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    for texts, labels in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = lstm(texts)\n",
    "\n",
    "        loss = loss_func(outputs, labels)\n",
    "        pred_labels = outputs.argmax(dim=1).tolist()\n",
    "\n",
    "        predictions.extend(pred_labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    \n",
    "    return epoch_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_epoch()\n",
    "        test_loss, pred_test = test_epoch()\n",
    "\n",
    "        print(f\"TRAIN Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
    "        print(f\"TEST Epoch {epoch+1}, Loss: {test_loss:.4f}\")\n",
    "\n",
    "        compute_metrics(y_test, pred_test)\n",
    "        print()\n",
    "    \n",
    "    return pred_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:31<00:00,  1.84s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 1, Loss: 1.1479\n",
      "TEST Epoch 1, Loss: 0.8183\n",
      "Macro Precision = 0.7066, Recall = 0.6819, F1 = 0.6846\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:28<00:00,  1.68s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 2, Loss: 0.7020\n",
      "TEST Epoch 2, Loss: 0.7368\n",
      "Macro Precision = 0.7478, Recall = 0.7562, F1 = 0.7491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:28<00:00,  1.69s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 3, Loss: 0.5903\n",
      "TEST Epoch 3, Loss: 0.5957\n",
      "Macro Precision = 0.8003, Recall = 0.7702, F1 = 0.7804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:29<00:00,  1.74s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 4, Loss: 0.5277\n",
      "TEST Epoch 4, Loss: 0.6012\n",
      "Macro Precision = 0.7851, Recall = 0.7864, F1 = 0.7819\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:30<00:00,  1.78s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 5, Loss: 0.4901\n",
      "TEST Epoch 5, Loss: 0.6092\n",
      "Macro Precision = 0.8117, Recall = 0.7643, F1 = 0.7757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLstmForClassification(\n",
       "  (lstm): LSTM(300, 300, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (ff): Linear(in_features=600, out_features=4, bias=True)\n",
       "  (embedding): Embedding(10480, 300)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = BiLstmForClassification(EMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES)\n",
    "lstm.init_embeddings(learn_embeddings=True, word2idx=word2idx)\n",
    "lstm.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:12<00:00,  4.26s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 1, Loss: 1.1721\n",
      "TEST Epoch 1, Loss: 0.7878\n",
      "Macro Precision = 0.7737, Recall = 0.6746, F1 = 0.6839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:19<00:00,  4.65s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 2, Loss: 0.5975\n",
      "TEST Epoch 2, Loss: 0.6124\n",
      "Macro Precision = 0.7741, Recall = 0.7895, F1 = 0.7678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:12<00:00,  4.27s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 3, Loss: 0.3817\n",
      "TEST Epoch 3, Loss: 0.5007\n",
      "Macro Precision = 0.8417, Recall = 0.8158, F1 = 0.8244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:56<00:00,  3.31s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 4, Loss: 0.2498\n",
      "TEST Epoch 4, Loss: 0.5869\n",
      "Macro Precision = 0.8119, Recall = 0.8201, F1 = 0.8013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:04<00:00,  3.79s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch 5, Loss: 0.1808\n",
      "TEST Epoch 5, Loss: 0.5399\n",
      "Macro Precision = 0.8547, Recall = 0.8320, F1 = 0.8397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не нужно забывать базу!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
